from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from typing import List, Dict
import torch, os
import re, requests
import time

def save_to_vectordb(codes: List[Dict[str, str]]):
  """
  ì½”ë“œ ì¡°ê°ì„ ë²¡í„° DBì— ì €ì¥í•˜ë˜, ë™ì¼í•œ íŒŒì¼(source)ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ 1.0ì¸ ë¬¸ì„œëŠ” ì œì™¸í•˜ê³  ì €ì¥.
  """
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

  # Chroma ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
  embedding = OpenAIEmbeddings()
  vectorstore = Chroma(
    collection_name="code_collection",
    persist_directory="chroma_db",
    embedding_function=embedding
  )

  new_documents = []
  
  for code in codes:
    source_name = code["name"]
    chunks = text_splitter.split_text(code["text"])

    for idx, chunk in enumerate(chunks):
      # í˜„ì¬ ì²­í¬ë¥¼ ë²¡í„°DBì—ì„œ ê²€ìƒ‰í•˜ì—¬ ë™ì¼í•œ source ë‚´ ìœ ì‚¬ë„ 1.0ì¸ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
      similar_docs = vectorstore.similarity_search_with_score(chunk, k=5)  # ìœ ì‚¬í•œ ë¬¸ì„œ ìµœëŒ€ 5ê°œ ê²€ìƒ‰

      has_duplicate = any(1 - score == 1.0 for _, score in similar_docs)  # ìœ ì‚¬ë„ 1.0 ì—¬ë¶€ í™•ì¸
      if has_duplicate:
        print(f"âš ï¸ ë™ì¼í•œ ì½”ë“œ ì²­í¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì €ì¥ ìƒëµ: {source_name} (chunk {idx})")
        continue  # ì¤‘ë³µì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
      
      # ê³ ìœ í•œ doc_id ìƒì„±
      doc_id = f"{source_name}_chunk_{idx}_{int(time.time() * 1000)}"
      doc_metadata = {
        "source": source_name,
        "chunk_id": idx,
        "doc_id": doc_id,
        "timestamp": int(time.time())
      }
      
      new_documents.append((chunk, doc_metadata))

  # ìƒˆë¡œìš´ ë¬¸ì„œë§Œ ì¶”ê°€ë¡œ ì €ì¥
  if new_documents:
    vectorstore.add_texts(
      texts=[doc for (doc, meta) in new_documents],
      metadatas=[meta for (doc, meta) in new_documents]
    )
    vectorstore.persist()
    print(f"âœ… {len(new_documents)}ê°œì˜ ì½”ë“œ ì²­í¬ê°€ ë²¡í„°DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
  else:
    print("âš ï¸ ìƒˆë¡œìš´ ì½”ë“œ ì²­í¬ê°€ ì—†ì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

def get_review_prompt(history, prev_codes, codes):
  prompt_template = """\
  ì´ì „ ëŒ€í™” ë‚´ì—­ ì…ë‹ˆë‹¤.
  {history}
  
  ì•„ë˜ëŠ” ì´ì „ ì½”ë“œ ì¡°ê°ë“¤ì…ë‹ˆë‹¤:
  {context}

  ê·¸ë¦¬ê³  ì•„ë˜ëŠ” ìƒˆë¡œ ì œì¶œëœ ì½”ë“œì…ë‹ˆë‹¤:
  {new_code}

  ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
  1. ìƒˆ ì½”ë“œê°€ ì´ì „ ì½”ë“œì— ë¹„í•´ ì–´ë–¤ ë¶€ë¶„ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ìš”ì•½
  2. ê°œì„ í•´ì•¼ í•  ì , ì ì¬ì  ë²„ê·¸, ë¦¬íŒ©í† ë§ í¬ì¸íŠ¸ ë“± ì½”ë“œ ë¦¬ë·° í¬ì¸íŠ¸ ì œì‹œ

  ë‹µë³€ì„ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.
  """

  context = '\n\n'.join(prev_codes)
  new_codes = '\n\n'.join([code['text'] for code in codes])
  
  template = PromptTemplate(
      template=prompt_template,
      input_variables=["history", "context", "new_code"]
  )
  final_prompt = template.format(history=history, context=context, new_code=new_codes)
  return final_prompt

def parse_codes(prompt):
  github_urls = []
  pattern = r'https://github\.com/(\S+)'
  matches = re.findall(pattern, prompt)
  if matches:
    for github_url in matches:
      github_url = 'https://raw.githubusercontent.com/' + github_url.replace('blob', 'refs/heads')
      
      if github_url not in github_urls:
        github_urls.append(github_url)
        
  codes = []
  for github_url in github_urls:
    split_text = github_url.split('/')
    name = split_text[-1]
    response = requests.get(github_url)
    codes.append(
      {
        'name': name,
        'text': response.text
      }
    )
  
  return codes

def search_prev_codes_from_vectordb(codes: List[Dict[str, str]]) -> List[str]:
  """
  í˜„ì¬ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ì™€ ìœ ì‚¬í•œ ì´ì „ ì½”ë“œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¬êµ¬ì„±.
  1) í˜„ì¬ ì½”ë“œì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ 1ë³´ë‹¤ ë‚®ì€ ë¬¸ì„œë“¤ì„ í•„í„°ë§
  2) ê°€ì¥ ìµœì‹ (timestampê°€ ë†’ì€) ì½”ë“œë¥¼ ì„ íƒ
  3) í•´ë‹¹ ì½”ë“œì˜ source ì´ë¦„ì„ ì´ìš©í•˜ì—¬ ë²¡í„°DBì—ì„œ ê´€ë ¨ ì½”ë“œ ì²­í¬ë¥¼ ê²€ìƒ‰
  4) chunk_id ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì½”ë“œë¡œ ì¬ì¡°í•©

  :param codes: {'name': str, 'text': str} í˜•íƒœì˜ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
  :return: ì´ì „ ì½”ë“œê°€ ì¬êµ¬ì„±ëœ ë¦¬ìŠ¤íŠ¸
  """
  prev_codes = []
  
  embedding = OpenAIEmbeddings()
  vectorstore = Chroma(
    collection_name="code_collection",
    persist_directory="chroma_db",
    embedding_function=embedding
  )

  for code in codes:
    source_name = code["name"]
    code_text = code["text"]

    # í˜„ì¬ ì½”ë“œì˜ ì„ë² ë”© ìƒì„±
    # query_embedding = embedding.embed_query(code_text)  # ğŸ”¥ `embed_query()` ì‚¬ìš©

    # ë²¡í„°DBì—ì„œ ìœ ì‚¬í•œ ì´ì „ ì½”ë“œ ê²€ìƒ‰ (ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ìµœëŒ€ 5ê°œ ê²€ìƒ‰)
    similar_docs = vectorstore.similarity_search_with_score(code_text, k=5)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ 1.0(ì™„ì „ ë™ì¼í•œ ì½”ë“œ)ë³´ë‹¤ ë‚®ì€ ê²ƒë§Œ í•„í„°ë§
    filtered_candidates = [
      (doc, score) for doc, score in similar_docs if 1 - score < 1.0
    ]

    if not filtered_candidates:
      continue

    # timestampê°€ ê°€ì¥ ë†’ì€ í›„ë³´ ì„ íƒ
    latest_candidate = max(filtered_candidates, key=lambda x: x[0].metadata["timestamp"])
    latest_source_name = latest_candidate[0].metadata["source"]

    # ë™ì¼í•œ source ì´ë¦„ì„ ê°€ì§„ ëª¨ë“  ì²­í¬ ê²€ìƒ‰
    related_docs = vectorstore.get(
      where={"source": latest_source_name},
      include=["documents", "metadatas"]
    )

    # ì²­í¬ë¥¼ chunk_id ìˆœìœ¼ë¡œ ì •ë ¬
    chunks_with_metadata = list(zip(related_docs["documents"], related_docs["metadatas"]))
    sorted_chunks = sorted(chunks_with_metadata, key=lambda x: x[1]["chunk_id"])

    # ì²­í¬ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ê²°í•©
    reconstructed_code = "\n".join([chunk for (chunk, meta) in sorted_chunks])

    prev_codes.append(reconstructed_code)

  return prev_codes