{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbUg0whSNNlJBzMZl4K70e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasier48/DeepLearning/blob/main/Pratice_Week_3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tqdm boto3 requests regex sentencepiece sacremoses datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2AznLgDy1ke",
        "outputId": "fc5683ac-4c11-4d57-cace-122b570e3565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.35.90-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.90 (from boto3)\n",
            "  Downloading botocore-1.35.90-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.90->boto3) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.90->boto3) (1.17.0)\n",
            "Downloading boto3-1.35.90-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.90-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, sacremoses, jmespath, fsspec, dill, multiprocess, botocore, s3transfer, boto3, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.35.90 botocore-1.35.90 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 jmespath-1.0.1 multiprocess-0.70.16 s3transfer-0.10.4 sacremoses-0.1.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqaefoaRyyGl",
        "outputId": "b2d9ca69-f8b5-41cd-814f-6bbc22cd7c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 데이터 확인: {'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.', 'hypothesis': 'Product and geography are what make cream skimming work. ', 'label': 1, 'idx': 0}\n",
            "라벨의 종류: ['entailment', 'neutral', 'contradiction']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')\n",
        "\n",
        "ds = load_dataset(\"glue\", \"mnli\")\n",
        "train_dataset = ds['train']\n",
        "print(f\"Train 데이터 확인: {train_dataset[0]}\")\n",
        "\n",
        "label_info = train_dataset.features['label']\n",
        "print(f\"라벨의 종류: {label_info.names}\")\n",
        "\n",
        "num_labels = len(label_info.names)\n",
        "\n",
        "dataset_length = 10000\n",
        "train_dataset = train_dataset.select(range(dataset_length))\n",
        "\n",
        "validation_matched_dataset = ds['validation_matched']\n",
        "\n",
        "test_dataset = ds['test_matched']\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label'])\n",
        "\n",
        "    # [MYCODE] 전제와 가설 문장을 합체\n",
        "    texts.append(row['premise'] + \" \" + row['hypothesis'])\n",
        "\n",
        "  encoding = tokenizer(texts, padding=True, truncation=False, max_length=max_len, return_tensors=\"pt\")\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return encoding.input_ids, labels, encoding.attention_mask\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "validation_loader = DataLoader(\n",
        "    validation_matched_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, n_epochs):\n",
        "  loss_list = []\n",
        "  train_acc_list = []\n",
        "  validation_acc_list = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0.\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:\n",
        "      model.zero_grad()\n",
        "      inputs, labels, attention_mask = data\n",
        "      inputs, labels, attention_mask = inputs.to(device), labels.to(device), attention_mask.to(device)\n",
        "\n",
        "      preds = model(inputs, attention_mask)\n",
        "      loss = loss_fn(preds, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      loss_list.append(total_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      train_acc = accuracy(model, train_loader)\n",
        "      train_acc_list.append(train_acc)\n",
        "\n",
        "      validation_acc = accuracy(model, validation_loader)\n",
        "      validation_acc_list.append(validation_acc)\n",
        "      print(f\"=========> Train acc: {train_acc:.3f} | Validation acc: {validation_acc:.3f}\")\n",
        "\n",
        "\n",
        "  return (loss_list, train_acc_list, validation_acc_list)\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels, attention_mask = data\n",
        "    inputs, labels, attention_mask = inputs.to(device), labels.to(device), attention_mask.to(device)\n",
        "\n",
        "    preds = model(inputs, attention_mask)\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class PreTraiendTextClassifier(nn.Module):\n",
        "  def __init__(self, num_labels, dropout_rate):\n",
        "    super(PreTraiendTextClassifier, self).__init__()\n",
        "\n",
        "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.classifier = nn.Linear(768, num_labels)\n",
        "\n",
        "  def forward(self, x, attention_mask):\n",
        "    x = self.encoder(x, attention_mask)['last_hidden_state']\n",
        "    x = self.dropout(x[:, 0])\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "\n",
        "fine_tunning_model = PreTraiendTextClassifier(num_labels, dropout_rate=0.1)\n",
        "\n",
        "device = torch.device('cpu')\n",
        "lr = 1e-5\n",
        "fine_tunning_model = fine_tunning_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(fine_tunning_model.parameters(), lr=lr, weight_decay=0.01)\n",
        "# optimizer = Adam(fine_tunning_model.parameters(), lr=lr)\n",
        "n_epochs = 1\n",
        "\n",
        "fine_tunning_result = train_model(fine_tunning_model, optimizer, n_epochs)\n",
        "\n",
        "# =============================================================================================>\n",
        "\n",
        "from transformers import DistilBertModel, DistilBertConfig\n",
        "\n",
        "# [MYCODE] Non-traiend 되지 않은 모델 정의\n",
        "class NonTrainedTextClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, config, dropout_rate):\n",
        "        super(NonTrainedTextClassifier, self).__init__()\n",
        "        self.encoder = DistilBertModel(config)  # 사전 학습되지 않은 DistilBERT\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        x = self.encoder(x, attention_mask)['last_hidden_state']\n",
        "        x = self.dropout(x[:, 0])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "non_trained_config = DistilBertConfig()\n",
        "non_traiend_text_classifier = NonTrainedTextClassifier(num_labels, non_trained_config, dropout_rate=0.1)\n",
        "non_traiend_text_classifier = non_traiend_text_classifier.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(fine_tunning_model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "non_trained_result = train_model(non_traiend_text_classifier, optimizer, n_epochs)"
      ],
      "metadata": {
        "id": "XyxZHEaezLkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbd9bc5-e34b-4778-a3af-e1106a526a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2691: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_acc(first_accs, second_accs, label1='train', label2='test'):\n",
        "  x = np.arange(len(first_accs))\n",
        "\n",
        "  plt.plot(x, first_accs, label=label1)\n",
        "  plt.plot(x, second_accs, label=label2)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "fine_tunning_train_acc_list = fine_tunning_result[1]\n",
        "fine_tunning_validation_acc_list = fine_tunning_result[2]\n",
        "plot_acc(fine_tunning_train_acc_list, fine_tunning_validation_acc_list, label1=\"Fine-Tunning-Acc\", label2=\"Validation-Acc\")\n",
        "\n",
        "non_trained_train_acc_list = non_trained_result[1]\n",
        "non_traiend_validation_acc_list = non_trained_result[2]\n",
        "plot_acc(non_trained_train_acc_list, non_traiend_validation_acc_list, label1=\"Non-Train-Acc\", label2=\"Validation-Acc\")\n",
        "\n",
        "fine_tunning_loss_list = fine_tunning_result[0]\n",
        "non_trained_loss_list = non_trained_result[0]\n",
        "plot_acc(fine_tunning_loss_list, non_trained_loss_list, label1=\"Fine-Tunning-Loss\", label2=\"Non-Train-Loss\")\n"
      ],
      "metadata": {
        "id": "IFFj54Gxip7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5X9GsUHqu8sg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}